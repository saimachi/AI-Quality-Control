{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dddf8fa0-a3c3-4e6b-943c-3b96255017bb",
   "metadata": {},
   "source": [
    "# PepsiCo Lab Potato Chips Quality Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdadaf3c-664f-4f6e-bddb-1dc8ddbef317",
   "metadata": {},
   "source": [
    "**Written by**: Sai Machiraju and Dylan Winer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be853f15-866a-4ac6-8a70-7971bfd4a33e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This dataset was provided by Frito-Lay, the subidiary of Pepsico, in an effort to improve their chip quality control through a Kaggle competition two years ago. We decided to tackle this challenge to sharpen our deep learning and computer vision skills. During the chip-making process, some of the chips get burnt, and there is a maximum amount of damage for a chip to be passable. Therefore, acting like PepsiCo quality control engineers, we built a deep learning model to solve this problem.\n",
    "[Link to Kaggle](https://www.kaggle.com/datasets/concaption/pepsico-lab-potato-quality-control/data \"Kaggle Data\")\n",
    "\n",
    "### Dataset\n",
    "The dataset consists of a balanced collection of 961 JPG images within defective and non-defective labels hosted on Kaggle. Therefore, our deep learning model needed to perform a binary classification to categorize images of chips as either defective or non-defective. \n",
    "\n",
    "### Introduction to PyTorch and TorchVision\n",
    "Computer vision enables machines to interpret and understand the visual world, providing a vital source of input data to train and test deep learning models. PyTorch is the most popular open-source deep learning library, which includes the TorchVision library dedicated to computer vision tasks. TorchVision is capable of a wide variety of powerful functionalities, ranging from image preprocessing to dataset handling to evaluation. PyTorch and TorchVision will serve as the backbone for our deep learning model, allowing us to build a robust and efficient binary classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d27dc5-f1f7-4ee2-9ccd-6758ea72371d",
   "metadata": {},
   "source": [
    "**First Attempt at Image Pre-Processing**\n",
    "In our original attempt to process the loading data, we tried to traverse the train and test folders, create `X_train` and `X_test` matrices and `y_train` and `y_test` vectors with all samples in the dataset, and load these vectors into Numpy matrices. However, it was extremely memory-intensive to immediately iterate through all samples in both the Train and Test folders, so we opted for a different approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9024ef-7737-4611-8ac7-b9b5cf8b341a",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Initialize X_train and X_test lists\n",
    "X_train = []\n",
    "X_test = []\n",
    "base_path = os.path.join(\"pepsico-lab-potato-quality-control\", \"Pepsico RnD Potato Lab Dataset\")\n",
    "\n",
    "# Encoding: 1 represents defective, 0 represents non-defective\n",
    "for folder in ['Train', 'Test']:\n",
    "    for category in ['Non-Defective', 'Defective']:\n",
    "        folder_path = os.path.join(base_path, folder, category)\n",
    "        for image_path in os.listdir(folder_path):\n",
    "            # Ignore the init files, only want jpg files\n",
    "            if image_path.endswith('.jpg') is False:\n",
    "                continue\n",
    "            img = Image.open(os.path.join(folder_path, image_path))\n",
    "            if folder == 'Train':\n",
    "                X_train.append(np.asarray(img))\n",
    "            else:\n",
    "                X_test.append(np.asarray(img))\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "print(f'X_test Shape: {X_test.shape}')\n",
    "print(f'X_train Shape: {X_train.shape}')\n",
    "\n",
    "with open('X_train.npy', 'wb') as file:\n",
    "    np.save(file, X_train)\n",
    "\n",
    "with open('X_test.npy', 'wb') as file:\n",
    "    np.save(file, X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c075f7eb-6781-4165-bb01-b62645a14666",
   "metadata": {},
   "source": [
    "**Background Removal Attempt**\n",
    "\n",
    "Based on previous experience with OpenCV, we elected to import the OpenCV library to aid in the image filtering and analysis process. The photos of the chips did not all have a consistent background, so we wanted to improve performance by converting the background to white. \n",
    "\n",
    "Therefore, we searched for background removal algorithms utilizing OpenCV methods, and we selected a function from FreedomVC. [Link to FreedomVC Site](https://www.freedomvc.com/index.php/2022/01/17/basic-background-remover-with-opencv/ \"FreedomVC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086bfbcb-ad18-4c88-af68-36262588dac2",
   "metadata": {},
   "source": [
    "**Background Removal Function**\n",
    "\n",
    "This function is designed to remove the background from an input image using color-based segmentation in the HSV color space. The process requires creating a binary mask that distinguishes between the foreground and the background based on the image's saturation and brightness channels. The input first converts the RGB space to the HSV (Hue, Saturation, Value) color space using cv2.cvtColor. After, the saturation mask is created by extracting the saturation channel (S). Next, a mask (s) is created where values below a threshold (80) are set to 0, and values above or equal to the threshold are then 1. It then increases the brightness of the Value channel (V) by adding 80 to each value. Next, a modulo of 255 is applied to ensure that values remain in the valid range of 0-255. We create a mask (v) where values above a threshold (80) are set to 1, and values below the threshold are set to 0. Combining the saturation and value masks (s and v) into a single binary mask allows us to create the foreground. Pixels are considered part of the foreground if either the saturation or brightness are above their thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb1fb7a-2394-4110-ba5d-f34b36cbf691",
   "metadata": {},
   "source": [
    "It inverts the foreground mask to obtain the background mask. Pixels that are not part of the foreground are set to 255 (white).  Utilizing cv2.bitwise_and to apply the foreground, we set the background pixels to 0 and kept the foreground constant. The function combines the foreground and background image, converting the final image to a NumPy _array (img_np)_ and returning it.\n",
    "\n",
    "_Note on Return_:\n",
    "\n",
    "The function originally returned an OpenCV image; however, after testing, we needed the function to return the image as an explicit Numpy array to prevent future errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0921ce25-367b-40e5-9582-626a43fba908",
   "metadata": {},
   "source": [
    "**Implementation**\n",
    "\n",
    "We attempted to pass each of the images through the bgremove function in the initialization step. However, after testing the accuracy with this inclusion, we realized the algorithm performed better without the removed background. Regardless, this attempt enhanced our understanding of the OpenCV library and was effective at segmenting the foreground and background of the chip images passed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4919b4-5f33-4138-b23a-a55579392f09",
   "metadata": {},
   "source": [
    "```python\n",
    "import cv2\n",
    "\n",
    "def bgremove(myimage):\n",
    "    # BG Remover\n",
    "    myimage_hsv = cv2.cvtColor(myimage, cv2.COLOR_BGR2HSV)\n",
    "     \n",
    "    #Take S and remove any value that is less than half\n",
    "    s = myimage_hsv[:,:,1]\n",
    "    s = np.where(s < 80, 0, 1) # Any value below 80 will be excluded\n",
    " \n",
    "    # We increase the brightness of the image and then mod by 255\n",
    "    v = (myimage_hsv[:,:,2] + 80) % 255\n",
    "    v = np.where(v > 80, 1, 0)  # Any value above 80 will be part of our mask\n",
    " \n",
    "    # Combine our two masks based on S and V into a single \"Foreground\"\n",
    "    foreground = np.where(s+v > 0, 1, 0).astype(np.uint8)  #Casting back into 8bit integer\n",
    " \n",
    "    background = np.where(foreground==0,255,0).astype(np.uint8) # Invert foreground to get background in uint8\n",
    "    background = cv2.cvtColor(background, cv2.COLOR_GRAY2BGR)  # Convert background back into BGR space\n",
    "    foreground=cv2.bitwise_and(myimage,myimage,mask=foreground) # Apply our foreground map to original image\n",
    "    finalimage = background+foreground # Combine foreground and background\n",
    "    img_np = np.array(finalimage)\n",
    "    return img_np\n",
    "    #return finalimage\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551cb344-6639-4704-bea9-df39c00846f1",
   "metadata": {},
   "source": [
    "**Model**\n",
    "\n",
    "We implemented the LeNet Convolutional Neural Network (CNN) architecture, using the code from [this PyImageSearch article](https://pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn/). One key modification, though, was that we excluded the final softmax layer, returning the logits (log probabilities) directly. We applied the softmax function in our training code.\n",
    "\n",
    "We chose LeNet because of its simplicity: We were able to reach 95% accuracy using a dataset with only 700 training examples. Additionally, LeNet is ubiquitous: LeNet-5 was first documented in a [transformative 1998 paper.](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)\n",
    "\n",
    ">**Note:** The LeNet model module is available [on our GitHub repository.](https://github.com/saimachi/AI-Quality-Control)\n",
    "\n",
    "**Libraries**\n",
    "\n",
    "We imported libraries to handle the complex tasks of image pre-processing, model creation, optimization, and implementation.\n",
    "\n",
    "* The os library in Python provides a way to interact with the operating system by offering functions for file and directory manipulation, path operations, and more, facilitating file management within our Jupyter Lab environment.\n",
    "* The torch library, part of the PyTorch framework, provides data structures for efficient multi-dimensional tensor computations and automatic differentiation for building and training neural networks.\n",
    "* BCEWithLogitsLoss (Binary Cross Entropy with Logits Loss)  is a loss function for binary classification. This loss function combines the sigmoid activation function with the binary cross-entropy loss.\n",
    "* Adam (Adaptive Moment Estimation), is an optimization algorithm for training deep neural networks. It combines RMSprop, which adapts learning rates for each parameter individually, and Momentum, which adds a moving average of past gradients to smooth the optimization trajectory. Adam maintains two moving averages for each parameter, which handles different learning rates for different parameters, making it well-suited for deep learning tasks.\n",
    "* Sigmoid (sigmoid activation function) is used to squash input values to a range between 0 and 1, which produces a likelihood of a sample belonging to a certain class (i.e., being defective).\n",
    "* PIL (Python Imaging Library or Pillow) opens, manipulates, and saves images. It has many image processing capabilities, including resizing, cropping, rotating, and filtering.\n",
    "* NumPy is a numerical computing library for Python used in various domains, including machine learning, data analysis, and numerical simulations. Its key feature is the numpy.array data structure, which allows efficient and fast operations on large datasets, including images converted to arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab1526-e646-48ec-ac29-f021053f4462",
   "metadata": {},
   "source": [
    "**Dataset Class**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a5d2d6-fb01-4670-875c-810a706c8803",
   "metadata": {},
   "source": [
    "**Constructor (`__init__`)**\n",
    "\n",
    "The constructor initializes attributes of the `Dataset` instance based on the provided parameters, setting up the initial state necessary for creating batches of training or validation data. Notably, we apply data augmentation transforms to the training dataset, but not the testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7418fe-2f77-47b2-9c72-568200f95154",
   "metadata": {},
   "source": [
    "**`__len__`**\n",
    "\n",
    "The `__len__` overloaded method returns the size or length of the dataset; in this case, it returns the number of samples in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3420b91e-2b00-43ee-963d-6aa25c4eb944",
   "metadata": {},
   "source": [
    "**`__getitem__`**\n",
    "\n",
    "The `__getitem__` overloaded method is designed to retrieve a specific image and label from the dataset based on its index. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85274e23-77d9-4085-aa49-7db0ff746c0c",
   "metadata": {},
   "source": [
    "**`generate_datset`**\n",
    "\n",
    "Its purpose is to create an instance of the `Dataset` class based on the directory structure of the PepsiCo RnD Potato Lab dataset. It distinguishes between training and testing datasets using the `is_train` parameter.\n",
    "\n",
    ">**Note:** We renamed `Test/Not Defective` in the Kaggle dataset download to `Test/Non-Defective` to simplify the data loading process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b073a9-28d8-461b-9fc1-7ef03fbc335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from lenet import LeNet\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import sigmoid\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Define a custom Dataset class for handling image data\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_ids, labels, base_path, is_training):\n",
    "        \"\"\"\n",
    "        Constructor for the Dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_ids : list[str]\n",
    "            List of file names (not paths)\n",
    "        labels : list[int]\n",
    "            List of class identifiers, corresponding to the list of file IDs\n",
    "        base_path : str\n",
    "            Path to the folder containing the files in `file_ids`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dataset\n",
    "        \"\"\"\n",
    "        self.file_ids = file_ids\n",
    "        self.labels = labels\n",
    "        self.base_path = base_path\n",
    "        # Define a list of image transformations using torchvision.transforms\n",
    "        transform_list = [\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(28)\n",
    "        ]\n",
    "        # If it is a training dataset, include additional data augmentation transformations\n",
    "        if is_training:\n",
    "            transform_list.extend([\n",
    "                transforms.RandomHorizontalFlip(), # Randomly flip the image horizontally\n",
    "                transforms.RandomVerticalFlip(), # Randomly flip the image vertically\n",
    "                transforms.RandomRotation(degrees=20), # Randomly rotate the image by up to 20 degrees\n",
    "                # Uncomment the line below to include color jittering\n",
    "                # However, from our tests, its inclusion worsened the model's accuracy\n",
    "                #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
    "            ])\n",
    "        # Add the final transformation to convert the image to a PyTorch tensor\n",
    "        transform_list.append(transforms.ToTensor())\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the size of the dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "        \"\"\"\n",
    "        return len(self.file_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a feature and label tuple based on the index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index : int\n",
    "            This function does not check the bounds of the Dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (torch.tensor, int) : Image vector and class\n",
    "        \"\"\"\n",
    "        # Get the filename and label for the given index\n",
    "        filename = self.file_ids[index]\n",
    "        label = self.labels[index]\n",
    "        # Read the image using torchvision.io.read_image\n",
    "        img = read_image(os.path.join(self.base_path, \"Defective\" if label == 1 else \"Non-Defective\", filename))\n",
    "        # Apply the defined transformations to the image\n",
    "        return self.transform(img), self.labels[index]\n",
    "\n",
    "\n",
    "# Function to generate training and testing datasets\n",
    "def generate_dataset(is_train):\n",
    "    \"\"\"\n",
    "    Generate training and testing datasets for the PepsiCo folder structure.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    is_train: bool\n",
    "        whether the dataset to be generated is for training (True) or testing (False).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dataset\n",
    "        Instance of the class representing the generated dataset\n",
    "    \"\"\"\n",
    "    # Root directory of the PepsiCo dataset\n",
    "    base_path = os.path.join(\"pepsico-lab-potato-quality-control\", \"Pepsico RnD Potato Lab Dataset\")\n",
    "    # Based on whether the dataset is for training or testing.   \n",
    "    train_path = os.path.join(base_path, \"Train\" if is_train else \"Test\")\n",
    "    file_ids = []\n",
    "    labels = []\n",
    "    \n",
    "     # Iterate through categories (Defective and Non-Defective) and collect filenames and labels\n",
    "    for category in [\"Defective\", \"Non-Defective\"]:\n",
    "        for filename in os.listdir(os.path.join(train_path, category)):\n",
    "            if filename.endswith(\".jpg\") is False:\n",
    "                continue\n",
    "            file_ids.append(filename)\n",
    "            labels.append(1 if category == \"Defective\" else 0)\n",
    "            \n",
    "    # Create and return a Dataset instance with the collected data\n",
    "    return Dataset(file_ids, labels, train_path, is_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c132d0c-7061-4d08-a815-d23b8e58dd1c",
   "metadata": {},
   "source": [
    "**Data Loaders**\n",
    "\n",
    "This portion of the code sets up data loaders for training, validation, and testing based on the generated datasets. It uses the PyTorch `DataLoader` and `Subset` classes to create iterable data loaders that provide mini-batches of data during the training and evaluation processes. Batches ensure that GPU memory is used efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0094dac2-7428-4b9c-b44b-c87936c22ea2",
   "metadata": {},
   "source": [
    "**Test and Validation Data Loader**\n",
    "\n",
    "The `generate_dataset()` function is called to create a testing dataset (`raw_test`) by setting `is_train=False`. The DataLoader is then used to create an iterable data loader (`test_loader`) for the testing dataset. It loads mini-batches of size 32, doesn't shuffle the data (`shuffle=False`), and utilizes pinned memory for faster data transfers from host memory to GPU VRAM (`pin_memory=True`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d45b3-45ff-492c-bf4b-b90d1575d4ed",
   "metadata": {},
   "source": [
    "**Training Data Loader**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9a4378-2bec-4a1b-b5e6-aebc638db202",
   "metadata": {},
   "source": [
    "A training dataset (`raw_train`) is generated by the same dataset call, but this time setting `is_train=True`. Random indices are generated using `random.sample()` to split the training dataset into 80% for training (`train_indices`) and 20% for validation (`val_indices`). Subsets (train and validation) of the training dataset are created using the `Subset` class based on the selected indices. After every training epoch, we run the validation set through the model to verify that it is not overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d53b00-a21b-4c27-96d3-9a849800de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader consisting of raw testing data in mini-batches of 32, without shuffling\n",
    "raw_test = generate_dataset(is_train=False)\n",
    "test_loader = DataLoader(raw_test, batch_size=32,shuffle=False,pin_memory=True)\n",
    "\n",
    "# Raw training data: split into 20% for validation and 80% for testing\n",
    "raw_train = generate_dataset(is_train=True)\n",
    "raw_train_len = len(raw_train)\n",
    "indices = random.sample(range(raw_train_len), raw_train_len)\n",
    "train_indices = indices[:int(raw_train_len * 0.8)]\n",
    "val_indices = list(set(indices).difference(set(train_indices)))\n",
    "train = Subset(raw_train, train_indices)\n",
    "validation = Subset(raw_train, val_indices)\n",
    "\n",
    "# Loaders for training and validation based on training images\n",
    "train_loader = DataLoader(train, batch_size=32, shuffle=True, pin_memory=True)\n",
    "validation_loader = DataLoader(validation, batch_size=32, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d375b-e619-423b-862a-2af873f1aed6",
   "metadata": {},
   "source": [
    "**Epochs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a357d24-c39f-4518-80c0-606a453bfc18",
   "metadata": {},
   "source": [
    "The number of epochs is a hyperparameter that we specified before training begins. It represents how many times the entire training dataset is processed by the model. Trial and error was required to determine the right number of epochs to ensure the model learns from the data without overfitting (learning noise in the data) or underfitting (not capturing the underlying patterns). We monitored the training and validation performance over epochs and stopped training when performance plateaued or started decreasing. After a few tests, we identified 10 as the optimal number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96852702-cfb7-45d4-8d1c-256324c27589",
   "metadata": {},
   "source": [
    "**Epoch Process**\n",
    "\n",
    "At the beginning of training, the model's parameters are initialized (weights and biases). The training dataset is divided into smaller batches to facilitate efficient computation and parameter updates. The training process is organized into a series of epochs. During each epoch: \n",
    "* The model is presented with each batch of 32 training examples sequentially\n",
    "* For each batch:\n",
    "  * The model computes predictions based on the current parameters\n",
    "  * The loss is calculated by comparing the model's predictions to the actual labels in the training data\n",
    "  * The optimizer adjusts the model's parameters to minimize the loss\n",
    "* The model is validated using data it hasn't been trained on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600bd45d-6496-44df-bb67-543cef0d5c92",
   "metadata": {},
   "source": [
    "**Training Phase**\n",
    "\n",
    "Within each epoch, the training data loader (`train_loader`) is iterated over in mini-batches. Model parameters are updated based on the computed loss and backpropagation.\n",
    "\n",
    "**Validation Phase**\n",
    "\n",
    "After each epoch, the model is evaluated on the validation dataset (`validation_loader`) without updating its parameters. The validation accuracy is computed to monitor the model's performance on unseen data during the training process.\n",
    "\n",
    "**Testing Phase**\n",
    "\n",
    "After completing all epochs, the model is evaluated on the test dataset (`test_loader`) without updating its parameters. The test set accuracy is calculated to assess the final performance of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3561d6-1d65-4924-9b1c-8586cead9c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the LeNet model for binary classification (3 input channels, 1 output channel)\n",
    "# this model architecture is designed for image classification tasks\n",
    "model = LeNet(3, 1)\n",
    "# A GPU device is set up\n",
    "gpu = torch.device(\"cuda\")\n",
    "# The LeNet model is moved to the GPU for faster computation during training and inference\n",
    "model = model.to(device=gpu)\n",
    "# Binary Cross Entropy with Logits Loss is chosen as the loss function, suitable for binary classification\n",
    "loss = BCEWithLogitsLoss()\n",
    "# The Adam optimizer is used to update the model parameters during the training process\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "# The training process is repeated for a specified number of epochs (10) \n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    # each batch contains 32 images\n",
    "    current_batch = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for batch in train_loader:\n",
    "        if current_batch % 4 == 0:\n",
    "            print(f\"Batch: {current_batch}\")\n",
    "        current_batch += 1\n",
    "        # batch[0] contains features, batch[1] contains labels\n",
    "        # Even though the labels are 0 or 1, they must be floats for BCEWithLogitsLoss()\n",
    "        X_batch, y_batch = batch[0].to(gpu), batch[1].to(gpu).unsqueeze(1).float()\n",
    "        output = model.forward(X_batch)\n",
    "        loss_value = loss(output, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "    # The training loss is printed after each epoch to monitor the model's convergence during training\n",
    "    print(f\"Train Loss: {loss_value}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    # Disable gradient computation (no backward pass)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        correct = 0\n",
    "        for val_batch in validation_loader:\n",
    "            X_validation, y_validation = val_batch[0].to(gpu), val_batch[1].to(gpu).unsqueeze(1)\n",
    "            val_preds = model.forward(X_validation)\n",
    "            # The final layer of the LeNet module does not apply the sigmoid function\n",
    "            correct += int(sum((sigmoid(val_preds) >= 0.5) == y_validation))\n",
    "        # Probability > 0.5 => Defective, otherwise non-defective\n",
    "        acc = correct * 100 / len(val_indices)\n",
    "        # The validation accuracy is printed after each epoch to assess the model's generalization to unseen data\n",
    "        print(f\"Validation Accuracy: {acc:.2f}%\")\n",
    "    print()\n",
    "    \n",
    "# Testing phase\n",
    "with torch.set_grad_enabled(False):\n",
    "    correct = 0\n",
    "    for test_batch in test_loader:\n",
    "        X_test, y_test = test_batch[0].to(gpu), test_batch[1].to(gpu).unsqueeze(1)\n",
    "        test_preds = model.forward(X_test)\n",
    "        correct += int(sum((sigmoid(test_preds) >= 0.5) == y_test))\n",
    "    # Probability > 0.5 => Defective, otherwise non-defective\n",
    "    acc = correct * 100 / len(raw_test)\n",
    "    # After completing all epochs, the test set accuracy is printed, providing a final evaluation of the model's \n",
    "    # performance on completely unseen data\n",
    "    print(f\"Test Set Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a00aab3-4208-4ebf-b636-338c296cbfbd",
   "metadata": {},
   "source": [
    "**Comment on Accuracy**\n",
    "\n",
    "The highest validation and test accuracies we recorded were 96.75% and 95.83% respectively, which demonstrates the model's ability to generalize and make predictions on unseen data examples. For the application of chip quality control, 95.83% represents a solid success rate that a company like PepsiCo could utilize to automate and streamline their production process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
